MLDB: Multi LoD DataBase : base de donnée non-Relationelle, non-Round-Robin, 
monotone, spécifique, à plusieurs niveaux de détails.


Les besoins en stockages d'APS sont particuliers :

- on stocke des données qui ne sont jamais plus modifiées
- on insère des dizaines de milliers de nouvelles données par minutes
- on veut garder un historique énorme
- on accepte de flouter les vieilles données pour gagner en perf/taille
- on veut pouvoir modifier légèrement le modèle de données sans perdre toutes 
  les données
- on va toujours requeter sur une plage de temps
- on n'a que des requètes paramétrées (pas de requétage libre)
- on a très peut de relation entre les données (aucune dans la pratique par
  simplicité)
- on ne stocke pas de données sensibles (une perte de donnée ne se traduit pas
  par des millions d'euros d'indemnitées).

Une base RRD satisfait tous ces points en apparence, mais elle ne permet de 
stoquer que des mesures, pas des évènements (ie elle stocke l'évolution de
variables).

Seules une base relationelle ou une base "maison" peuvent répondre à ses 
critères.

APS utilise une base relationelle ; tout autre choix initial aurait été 
suicidaire. Pourtant il s'agit d'un usage presque détourné d'une base 
relationelle, qui sont optimisées pour les relations (le cas compliqué), ce 
qui entraine un manque d'efficacité (lenteur lors du requétage et de 
l'insertion, énorme consomation de RAM), et des lourdeurs dans le code qui 
avec le temps se sont gangrénées.

On se demande donc souvent, désormais qu'avec l'expérience acquise on se 
représente clairement l'horizon des besoins, s'il ne serait pas possible de 
développer une base de données spécifique.

Evidement, autant le choix de la base relationelle à utiliser est consensuel 
(Oracle ?  Trop cher. MSSQL ? Pas sérieux. Mysql ? Gadget. PostgresQL, 
évidement), autant face à ce terrain vierge les opinions divergent sur la voie 
à suivre.

Il ne s'agit donc ici que d'explorer un petit secteur de ce vaste domaine
afin d'alimenter le débat. Volontairement, les aspects les plus oposés à APS 
sont ceux qui ont étés le plus développés, par goût de la nouveauté, de la 
contradiction, mais aussi par manque d'intérêt pour tout ce qu'APS fait déjà.  
Donc, si on s'attarde surtout sur des vues que n'offrent pas APS, ce n'est pas 
forcément parceque ce modèle de stoquage ne permet pas de les refaire ni 
forcément que l'anciene base ne permet pas de les obtenir.

* Fonctionnement général

L'idée de départ est la suivante : nous insérons linéairement et nous lisons 
généralement linéairement (notre modèle de données n'est pas relationnel, même 
s'il gagnerait à l'être un peu - références des transactions applicatives vers 
les flux réseau, ie d'http vers tcp par exemple). On va donc n'implémenter que
ce cas là. Un disque moderne lisant au minimum à 100Mo/s, une requète 
nécessitant de scanner 1Go d'historique tournera pendant maximum 10s (si on ne 
fait qu'une passe), et sans avoir besoin de cacher quoi que ce soit.

L'idée est donc de lire/écrire des fichiers plats.

Ensuite, il y a deux approches simples :

- Un seul fichier qui boucle, mais on ne peut pas écrire en append (donc 
  nécessite un lock) et sa taille étant fixe on ne peut pas garantir la durée
  de l'historique (impossible, par exemple, d'augmenter la capacité de 
stockage)

- Plusieurs fichiers qui se succèdent, avec effacement des vieux fichiers
lorsque l'historique est trop vieux ou qu'on manque de place

Le choix est vite fait.

A ce type de stockage on peut gratuitement ajouter un index : au lieux d'une 
seule séquence de fichiers on en a N et on choisit un paramètre dont le hash 
donnera le numéro de la séquence à lire ou écrire. Ici le paramètre qui semble 
le plus intéressant pour nous c'est l'IP serveur (quand on en a une).

Ensuite, on peut presque gratuitement ajouter une deuxième aide à la recherche 
: pour chaque fichier d'une séquence on peut stoquer le min et le max de 
chaque valeurs (lorsque ca a un sens). Ici, voulant optimiser l'écriture (ce 
qui est en fait inutile), j'ai choisi la souplesse : chaque base gère ces 
méta-données elle même et est donc libre de ne considérer qu'un ou certains 
champs. Ici, le champ vraiment intéressant c'est évidement le temps.

Donc, l'écriture d'un enregistrement se fait donc de la sorte :

- calcul du hash h en fonction de l'enregistrement (ici, hash de l'IP serveur)
- ajout de l'enregistrement dans le dernier fichier de la h-ième séquence 
  (faire un nouveau fichier si le précédent est trop grand / trop vieux)
- mise à jour simultanée des méta données de ce fichier pour tenir compte de 
  cette nouvelle entrée

Pour simplifier la mise à jour simultanée des méta données, dans cette 
première version on se limite à un seul écrivain. Il serait trivial d'ajouter 
un verroue sur ce fichier, d'autant qu'on peu s'arranger pour qu'il y ait 
assez peut d'écriture (lorsqu'on écrit une nouvelle date max, ajouter quelques 
secondes...)

La recherche se fait de la sorte :

- si on cherche en fonction d'un champ utilisé par l'index, trouver la ou les 
  numéro de séquence correspondant (sinon, les prendre tous)
- pour chacune de ces séquence, itérer sur tous les fichiers (ces itérations 
  peuvent se faire dans n'importe quel ordre)
- pour chaque fichier, inspecter les méta données. Si elle sont compatible 
  avec ce qui est recherché, alors scanner le fichier.

Cette opération peut, très facilement sans aucun lock, être exécuté 
simultanément par plusieurs threads.

* Différents niveaux de détail

On réalise simplement plusieurs nivaux de détail dans plusieurs base 
distinctes. Dans ce test c'est le même programme qui insère dans toutes les 
différents LoD par soucis d'économie et pour pouvoir utiliser le résultat du 
LoD N-1 pour fabriquer le LoD N (plutot que de repartir des points initiaux).  
Il m'a semblé en effet que ce serait couteux de fabriquer le LoD annuel qui 
aggrège par jour par exemple avec les mesures initiales (car si on a 1000
enregistrements par secondes cela ferait 1000 cumuls à faire par LoD (et dans 
un cumul il y a aussi le floutage, qui peut être couteux (par exemple, 
remplacer une IP par une zone...).

Mais l'économie réalisée est peut être négligeable (pas encore mesuré car pour 
l'instant mon floutage est minimaliste). Cela dit ca permettra toujours à 
l'avenir d'avoir des fonctions d'aggrégation/floutage aussi compliquées qu'on 
le souhaite, sans crainte.

Pour chaque métrique on a donc plusieurs base (une par LoD), et pour chaque 
LoD plusieurs séquences (une par entrées dans le hash) et pour chaque séquence
plusieurs fichiers.

La base de volumétrie, sur 3 niveaux (1 minute, 10 minutes et 1 heure), pour 
les 40Go de Lille (10 minutes de trafic (*)) consomme environ 500Mo (372Mo, 
65M, 60M). C'est beaucoup plus que pour APS car ici toutes les sockets sont 
stoquées (avec ports client) de façon à avoir autant d'information de 
volumétrie que netflow.

(*): Si on avait plus de 10 minutes de trafic c'est surtout Lod0 qui 
grossierait, presque proportionnellement, donc dans les 20G par jour (en 
considérant qu'on a les 10 minutes de pointe), soit pour trois jours 60G
(au delà de 3 jours on ne concerve pas ce LoD a priori).

1.3) Comparaison avec la base nova

optimisation du seqscan : fragmentation, parallelisation, cache
(posix_advise).

pas d'upgrade, chaque enregistrement comporte sa version (+ garantie 
d'exhaustivité du pattern matching).

pas de contrainte d'insérer dans l'ordre chronologique (meme si plus efficace
car temps utilisé en meta en général). Cf animation du front time.

L'absence de "front time" rend aussi le système plus robuste vis à vis
des petits tracas du quotidients, tel bons dans le temps, corruption de 
fichiers, déconnection de pollers, etc. En fait la base est toujours 
cohérente, au pire en effacant des fichiers.

Ideal pour le mode distribué, mais il y a mieux:

possibilité de distribuer les seqscans sur plusieurs pollers.

grande facilité pour effacer (rm) ou compresser (mv ds autre partition)
l'historique.

pas de construction d'aggrégats au fur et à mesure (cette monstruosité)
mais insertion simultanée dans tous les LoD, avec pour avantage de tout
simplifier (on peut meme supprimer un LoD intermédiaire si on veut) et
la base ne se floute pas toute seule avec le temps ! Le tout pour une
taille évidement négligeable, vu que le recouvrement de deux LoD est
la taille de lod n dans les lod n+1,2,etc.

Evidemment plus robuste vis à vis de la corruption de données qu'une
base relationelle (réparable en shell, au pire en supprimant le fichiers
qui pose pb).

pb: enregistrements de taille variable car plus simple, stress du GC 
générationnel car besoin de créer énormément d'objs immédiatement oubliés.
Pas de typage du contenu sérialisé (besoin du checker dynamiquement qu'on
ne se trompe pas de fichier).

Inconvénient majeur : on a que deux indexs (plutot un index et un 
accélérateur). On ne peut donc rechercher des entrées selon un critère
quelconque qu'au prix d'un scan complet. Il faut donc prévoir initialement
tous les cas d'usage présent _et_à_venir_. C'est probablement la raison pour 
laquelle il a semblé plus prudent de partir sur une base relationelle.

1.5) Comparaison avec RRDTool.

Pas besoin de fréquence fixe prédéfinie (et donc pas besoin d'interpoler
les données lors de l'insertion - en contrepartie, on interpole lors du
rendu graphique éventuel).

Plus de souplesse pour les fonctions de consolidation et surtout pour le choix
du flush (en contrepartie, il faut coder quelque lignes)

Lorsqu'on consolide les points, on peut aussi les dégrader !

Les bases RRDTool sont round-robin, c'est à dire qu'elles bouclent sur
elles même arrivé à la fin : avantage: ca ne grossit pas ; inconvénient: ca ne 
grossit pas.

Du fait des contraintes suplémentaires, RRDTool peut choisir tout seul le
RRA (Round-Robin Archive - notre LoD) en fonction des plages de temps 
demandées.

Un RRA ne peut pas contenir des évènements discrets ; il ne peut servir qu'à
contenir des mesures. C'est donc parfait pour la volumétrie mais inutilisable
pour des évènements qui ont une date de début, une date de fin, et d'autres
caractéristiques spécifiques que l'on souhaite conserver en partie.

2) L'IHM web

Génération de SVG soit sur le client avec Google Charts, soit sur le serveur
pour la vue en cercle par exemple.
Il est plus avantageux, dans ce domaine qui touche au coeur de métier 
(collecte, visualisation, stockage) de refaire les choses soit même.

Cf Joel on software http://www.joelonsoftware.com/articles/fog0000000007.html


2) Utilisation en monitoring

Possibilité d'utiliser autant de filtres que l'on souhaite gratuitement 
(puisqu'on part du postulat qu'on scannera toutes les données, alors que pour 
une base relationnelle on cherche en permanence un échapatoire à ce 
comportement "dégradé").

3) Utilisation en troubleshooting

4) Utilisation en archivage

(Note: le max des disques mécaniques lit à 100-150Mo/s, en SSD on trouve à 
250Mo/s mais c'est plus cher!).
http://www.tomshardware.com/reviews/ssd-upgrade-hard-drive,2956-5.html

Taille maxi de la base, à 100Mo/s, si on ne souhaite pas attendre plus de 3 
minutes un seq scan complet : 18Gb par niveaux d'aggrégat et par métrique.
Soit pour une dizaine de métriques sur 3 niveaux : dans les 500Gb.
Il faudrait idéalement un outils pour partitionner cet espace en réglant les
durées de vie des niveaux et l'effondrement des données.

